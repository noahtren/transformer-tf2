## Tokenizer: [SentencePiece](https://github.com/google/sentencepiece)

## Datasets

* wiki40b: cleaned dataset provided by Google and accessed through TensorFlow-datasets.

## Models

Transformer from Vaswani et. al
* positional encoding
* multi-head self-attention
* "boom" linear layer
